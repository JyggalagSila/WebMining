{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d27c72-31fc-47ac-ab3a-5247497690ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I'd have responded, if I were going</td>\n",
       "      <td>I'd have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn't they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I've wondered about rake to.  The client has ...</td>\n",
       "      <td>, don't force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I'd have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn't they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I've wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I'd have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don't force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27480 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic imports and loading of data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "original = pd.read_csv('TweetsNR.csv').dropna()\n",
    "dataframe = original.copy()\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72a330f-6dbf-4519-ac9c-f7584cdda4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27480 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# basic info\n",
    "\n",
    "original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e7b681-942a-4147-a01a-b1011b60a617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'd have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn't they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I've wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0                    I'd have responded, if I were going   neutral\n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                              my boss is bullying me...  negative\n",
       "3                         what interview! leave me alone  negative\n",
       "4       Sons of ****, why couldn't they put them on t...  negative\n",
       "...                                                  ...       ...\n",
       "27476   wish we could come see u on Denver  husband l...  negative\n",
       "27477   I've wondered about rake to.  The client has ...  negative\n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive\n",
       "27479                         But it was worth it  ****.  positive\n",
       "27480     All this flirting going on - The ATG smiles...   neutral\n",
       "\n",
       "[27480 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe for work\n",
    "\n",
    "dataframe = original.copy()\n",
    "dataframe.drop(['textID', 'selected_text'], axis=1, inplace=True)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbddbe7e-a653-418a-a211-363fa95ca134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>it was a biligual sweatshop LOL I talk 2 him ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6422</th>\n",
       "      <td>On the bus to NYC   http://yfrog.com/08kaifj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17769</th>\n",
       "      <td>: Ok its suppose 2b followfriday not unfollow ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16344</th>\n",
       "      <td>had such a fun time with allegra tonite!!! we ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15836</th>\n",
       "      <td>Very cute - I don't think I can make it to Ma...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>dunno. Maybe the flu. I feel a bitbetter now.</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11403</th>\n",
       "      <td>ya i did i seen all them but Robert</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22643</th>\n",
       "      <td>I was the blue  lol http://twitpic.com/67zgz</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Waking up early to go to the gym</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19474</th>\n",
       "      <td>it drained my energy</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "1315    it was a biligual sweatshop LOL I talk 2 him ...  negative   \n",
       "6422        On the bus to NYC   http://yfrog.com/08kaifj   neutral   \n",
       "17769  : Ok its suppose 2b followfriday not unfollow ...  positive   \n",
       "16344  had such a fun time with allegra tonite!!! we ...  positive   \n",
       "15836   Very cute - I don't think I can make it to Ma...   neutral   \n",
       "18695      dunno. Maybe the flu. I feel a bitbetter now.  positive   \n",
       "11403                ya i did i seen all them but Robert   neutral   \n",
       "22643       I was the blue  lol http://twitpic.com/67zgz   neutral   \n",
       "1332                    Waking up early to go to the gym   neutral   \n",
       "19474                               it drained my energy  negative   \n",
       "\n",
       "       sentiment_num  \n",
       "1315              -1  \n",
       "6422               0  \n",
       "17769              1  \n",
       "16344              1  \n",
       "15836              0  \n",
       "18695              1  \n",
       "11403              0  \n",
       "22643              0  \n",
       "1332               0  \n",
       "19474             -1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment data into numbers for machine learning\n",
    "\n",
    "dataframe['sentiment_num'] = dataframe['sentiment'].replace(['negative','neutral','positive'], [-1, 0, 1])\n",
    "dataframe.sample(n=10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "833c8353-5842-488d-a3ed-7e56550253fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it was a biligual sweatshop LOL I talk 2 him once in a while but not as much, he got an r6 - {'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.3108}\n",
      "Actual label: -1 / Predicted label: 1 <WRONG>\n",
      "\n",
      "On the bus to NYC   http://yfrog.com/08kaifj - {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      ": Ok its suppose 2b followfriday not unfollow Friday  aw well I have nice tweeters anyway! <-almost doesnt sound right...lol;) - {'neg': 0.0, 'neu': 0.67, 'pos': 0.33, 'compound': 0.75}\n",
      "Actual label: 1 / Predicted label: 1 <CORRECT>\n",
      "\n",
      "had such a fun time with allegra tonite!!! we saw 17again!! good movie - {'neg': 0.0, 'neu': 0.576, 'pos': 0.424, 'compound': 0.811}\n",
      "Actual label: 1 / Predicted label: 1 <CORRECT>\n",
      "\n",
      " Very cute - I don't think I can make it to MakerFaire, sadly - {'neg': 0.199, 'neu': 0.568, 'pos': 0.234, 'compound': 0.1263}\n",
      "Actual label: 0 / Predicted label: 1 <WRONG>\n",
      "\n",
      " dunno. Maybe the flu. I feel a bitbetter now. - {'neg': 0.302, 'neu': 0.698, 'pos': 0.0, 'compound': -0.3818}\n",
      "Actual label: 1 / Predicted label: -1 <WRONG>\n",
      "\n",
      " ya i did i seen all them but Robert - {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      "I was the blue  lol http://twitpic.com/67zgz - {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.4215}\n",
      "Actual label: 0 / Predicted label: 1 <WRONG>\n",
      "\n",
      "Waking up early to go to the gym - {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      " it drained my energy - {'neg': 0.379, 'neu': 0.303, 'pos': 0.318, 'compound': -0.1027}\n",
      "Actual label: -1 / Predicted label: -1 <CORRECT>\n",
      "\n",
      "Accuracy of VADER: 0.6354075691411936\n",
      "Evaluation of VADER:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.59      0.64      7781\n",
      "           0       0.68      0.49      0.57     11117\n",
      "           1       0.57      0.87      0.69      8582\n",
      "\n",
      "    accuracy                           0.64     27480\n",
      "   macro avg       0.65      0.65      0.63     27480\n",
      "weighted avg       0.65      0.64      0.63     27480\n",
      "\n",
      "Confusion matrix of Vader:\n",
      " Predicted    -1     0     1\n",
      "Actual                     \n",
      "-1         4552  1692  1537\n",
      " 0         1616  5480  4021\n",
      " 1          320   833  7429\n"
     ]
    }
   ],
   "source": [
    "# rule based sentiment analysis - VADER\n",
    "\n",
    "from nltk import sentiment\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "tokens = dataframe['text'].astype(str)\n",
    "\n",
    "VA = sentiment.vader.SentimentIntensityAnalyzer()\n",
    "all_vs = []\n",
    "for sentence in tokens:\n",
    "    vs = VA.polarity_scores(sentence)\n",
    "    all_vs.append(vs['compound'])\n",
    "    \n",
    "all_vs = [1 if num>=0.1 else -1 if num<=-0.1 else 0 for num in all_vs]\n",
    "\n",
    "examples = [1315, 6422, 17769, 16344, 15836, 18695, 11403, 22643, 1332, 19474]\n",
    "for num in examples:\n",
    "    print(f'{tokens[num]} - {VA.polarity_scores(tokens[num])}')\n",
    "    actual = dataframe['sentiment_num'][num]\n",
    "    predicted = all_vs[num-1]\n",
    "    print(f'Actual label: {actual} / Predicted label: {predicted} {\"<CORRECT>\" if actual == predicted else \"<WRONG>\"}\\n')\n",
    "\n",
    "acc_va = accuracy_score(dataframe[\"sentiment_num\"], all_vs)\n",
    "print(f'Accuracy of VADER: {acc_va}')\n",
    "print(f'Evaluation of VADER:\\n {classification_report(dataframe[\"sentiment_num\"], all_vs)}')\n",
    "print(f'Confusion matrix of Vader:\\n {pd.crosstab(dataframe[\"sentiment_num\"], all_vs, rownames=[\"Actual\"], colnames=[\"Predicted\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "10b4d5a4-9da5-4865-9eb2-e5b992a65616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it was a biligual sweatshop LOL I talk 2 him once in a while but not as much, he got an r6 - 0.5\n",
      "Actual label: -1 / Predicted label: 1 <WRONG>\n",
      "\n",
      "On the bus to NYC   http://yfrog.com/08kaifj - 0.0\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      ": Ok its suppose 2b followfriday not unfollow Friday  aw well I have nice tweeters anyway! <-almost doesnt sound right...lol;) - 0.475\n",
      "Actual label: 1 / Predicted label: 1 <CORRECT>\n",
      "\n",
      "had such a fun time with allegra tonite!!! we saw 17again!! good movie - 0.5385091145833333\n",
      "Actual label: 1 / Predicted label: 1 <CORRECT>\n",
      "\n",
      " Very cute - I don't think I can make it to MakerFaire, sadly - 0.07500000000000001\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      " dunno. Maybe the flu. I feel a bitbetter now. - 0.0\n",
      "Actual label: 1 / Predicted label: 0 <WRONG>\n",
      "\n",
      " ya i did i seen all them but Robert - 0.0\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      "I was the blue  lol http://twitpic.com/67zgz - 0.4\n",
      "Actual label: 0 / Predicted label: 1 <WRONG>\n",
      "\n",
      "Waking up early to go to the gym - 0.1\n",
      "Actual label: 0 / Predicted label: 1 <WRONG>\n",
      "\n",
      " it drained my energy - 0.0\n",
      "Actual label: -1 / Predicted label: 0 <WRONG>\n",
      "\n",
      "Accuracy of TextBlob: 0.5925036390101892\n",
      "Evaluation of TextBlob:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.40      0.51      7781\n",
      "           0       0.57      0.59      0.58     11117\n",
      "           1       0.57      0.77      0.66      8582\n",
      "\n",
      "    accuracy                           0.59     27480\n",
      "   macro avg       0.61      0.59      0.58     27480\n",
      "weighted avg       0.61      0.59      0.58     27480\n",
      "\n",
      "Confusion matrix of TextBlob:\n",
      " Predicted    -1     0     1\n",
      "Actual                     \n",
      "-1         3136  3139  1506\n",
      " 0         1112  6512  3493\n",
      " 1          261  1687  6634\n"
     ]
    }
   ],
   "source": [
    "# rule based sentiment analysis - TextBlob\n",
    "\n",
    "import textblob\n",
    "\n",
    "all_ts = []\n",
    "for sentence in tokens:\n",
    "    ts = textblob.TextBlob(sentence).sentiment.polarity\n",
    "    all_ts.append(ts)\n",
    "    \n",
    "all_ts = [1 if num>=0.1 else -1 if num<=-0.1 else 0 for num in all_ts]\n",
    "\n",
    "examples = [1315, 6422, 17769, 16344, 15836, 18695, 11403, 22643, 1332, 19474]\n",
    "for num in examples:\n",
    "    print(f'{tokens[num]} - {textblob.TextBlob(tokens[num]).sentiment.polarity}')\n",
    "    actual = dataframe['sentiment_num'][num]\n",
    "    predicted = all_ts[num-1]\n",
    "    print(f'Actual label: {actual} / Predicted label: {predicted} {\"<CORRECT>\" if actual == predicted else \"<WRONG>\"}\\n')\n",
    "\n",
    "acc_tb = accuracy_score(dataframe[\"sentiment_num\"], all_ts)\n",
    "print(f'Accuracy of TextBlob: {acc_tb}')\n",
    "print(f'Evaluation of TextBlob:\\n {classification_report(dataframe[\"sentiment_num\"], all_ts)}')\n",
    "print(f'Confusion matrix of TextBlob:\\n {pd.crosstab(dataframe[\"sentiment_num\"], all_ts, rownames=[\"Actual\"], colnames=[\"Predicted\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "83014ef3-a958-4112-8fef-9c1f0d991197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it was a biligual sweatshop LOL I talk 2 him once in a while but not as much, he got an r6 - 3.0\n",
      "Actual label: -1 / Predicted label: 1 <WRONG>\n",
      "\n",
      "On the bus to NYC   http://yfrog.com/08kaifj - 0.0\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      ": Ok its suppose 2b followfriday not unfollow Friday  aw well I have nice tweeters anyway! <-almost doesnt sound right...lol;) - 8.0\n",
      "Actual label: 1 / Predicted label: 1 <CORRECT>\n",
      "\n",
      "had such a fun time with allegra tonite!!! we saw 17again!! good movie - 7.0\n",
      "Actual label: 1 / Predicted label: 1 <CORRECT>\n",
      "\n",
      " Very cute - I don't think I can make it to MakerFaire, sadly - 0.0\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      " dunno. Maybe the flu. I feel a bitbetter now. - -2.0\n",
      "Actual label: 1 / Predicted label: -1 <WRONG>\n",
      "\n",
      " ya i did i seen all them but Robert - 0.0\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      "I was the blue  lol http://twitpic.com/67zgz - 3.0\n",
      "Actual label: 0 / Predicted label: 1 <WRONG>\n",
      "\n",
      "Waking up early to go to the gym - 0.0\n",
      "Actual label: 0 / Predicted label: 0 <CORRECT>\n",
      "\n",
      " it drained my energy - -2.0\n",
      "Actual label: -1 / Predicted label: -1 <CORRECT>\n",
      "\n",
      "Accuracy of AFINN: 0.6475254730713246\n",
      "Evaluation of AFINN:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.60      0.65      7781\n",
      "           0       0.69      0.52      0.60     11117\n",
      "           1       0.58      0.85      0.69      8582\n",
      "\n",
      "    accuracy                           0.65     27480\n",
      "   macro avg       0.66      0.66      0.65     27480\n",
      "weighted avg       0.66      0.65      0.64     27480\n",
      "\n",
      "Confusion matrix of AFINN:\n",
      " Predicted    -1     0     1\n",
      "Actual                     \n",
      "-1         4665  1633  1483\n",
      " 0         1551  5828  3738\n",
      " 1          319   962  7301\n"
     ]
    }
   ],
   "source": [
    "# rule based sentiment analysis - AFINN\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) \n",
    "\n",
    "all_afs = []\n",
    "for sentence in tokens:\n",
    "    afs = afn.score(sentence)\n",
    "    all_afs.append(afs)\n",
    "    \n",
    "all_afs = [1 if num>=1 else -1 if num<=-1 else 0 for num in all_afs]\n",
    "\n",
    "examples = [1315, 6422, 17769, 16344, 15836, 18695, 11403, 22643, 1332, 19474]\n",
    "for num in examples:\n",
    "    print(f'{tokens[num]} - {afn.score(tokens[num])}')\n",
    "    actual = dataframe['sentiment_num'][num]\n",
    "    predicted = all_afs[num-1]\n",
    "    print(f'Actual label: {actual} / Predicted label: {predicted} {\"<CORRECT>\" if actual == predicted else \"<WRONG>\"}\\n')\n",
    "\n",
    "acc_afn = accuracy_score(dataframe[\"sentiment_num\"], all_afs)\n",
    "print(f'Accuracy of AFINN: {acc_afn}')\n",
    "print(f'Evaluation of AFINN:\\n {classification_report(dataframe[\"sentiment_num\"], all_afs)}')\n",
    "print(f'Confusion matrix of AFINN:\\n {pd.crosstab(dataframe[\"sentiment_num\"], all_afs, rownames=[\"Actual\"], colnames=[\"Predicted\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e7121ada-801e-456e-ad2f-8f2a5ae73db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VADER</td>\n",
       "      <td>0.635408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TextBlob</td>\n",
       "      <td>0.592504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFINN</td>\n",
       "      <td>0.647525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Accuracy\n",
       "0     VADER  0.635408\n",
       "1  TextBlob  0.592504\n",
       "2     AFINN  0.647525"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rb = pd.DataFrame([['VADER', acc_va],\n",
    "                        ['TextBlob', acc_tb],\n",
    "                        ['AFINN', acc_afn]],\n",
    "                       columns=['Model', 'Accuracy'])\n",
    "results_rb.to_csv('results_rb.csv')\n",
    "results_rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa3a147-50e6-4ae3-a194-21334f97d592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "      <th>Tweet Tokenizer</th>\n",
       "      <th>Word Tokenize</th>\n",
       "      <th>Casual Tokenize</th>\n",
       "      <th>Twokenize</th>\n",
       "      <th>Twikenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>it was a biligual sweatshop LOL I talk 2 him ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[it, was, a, biligual, sweatshop, LOL, I, talk...</td>\n",
       "      <td>[it, was, a, biligual, sweatshop, LOL, I, talk...</td>\n",
       "      <td>[it, was, a, biligual, sweatshop, LOL, I, talk...</td>\n",
       "      <td>[it, was, a, biligual, sweatshop, LOL, I, talk...</td>\n",
       "      <td>[it, was, a, biligual, sweatshop, LOL, I, talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6422</th>\n",
       "      <td>On the bus to NYC   http://yfrog.com/08kaifj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[On, the, bus, to, NYC, http://yfrog.com/08kaifj]</td>\n",
       "      <td>[On, the, bus, to, NYC, http, :, //yfrog.com/0...</td>\n",
       "      <td>[On, the, bus, to, NYC, http://yfrog.com/08kaifj]</td>\n",
       "      <td>[On, the, bus, to, NYC, http://yfrog.com/08kaifj]</td>\n",
       "      <td>[On, the, bus, to, NYC, http, :, /, /, yfrog, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17769</th>\n",
       "      <td>: Ok its suppose 2b followfriday not unfollow ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[:, Ok, its, suppose, 2b, followfriday, not, u...</td>\n",
       "      <td>[:, Ok, its, suppose, 2b, followfriday, not, u...</td>\n",
       "      <td>[:, Ok, its, suppose, 2b, followfriday, not, u...</td>\n",
       "      <td>[:, Ok, its, suppose, 2b, followfriday, not, u...</td>\n",
       "      <td>[:, Ok, its, suppose, 2b, followfriday, not, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16344</th>\n",
       "      <td>had such a fun time with allegra tonite!!! we ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[had, such, a, fun, time, with, allegra, tonit...</td>\n",
       "      <td>[had, such, a, fun, time, with, allegra, tonit...</td>\n",
       "      <td>[had, such, a, fun, time, with, allegra, tonit...</td>\n",
       "      <td>[had, such, a, fun, time, with, allegra, tonit...</td>\n",
       "      <td>[had, such, a, fun, time, with, allegra, tonit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15836</th>\n",
       "      <td>Very cute - I don't think I can make it to Ma...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[Very, cute, -, I, don't, think, I, can, make,...</td>\n",
       "      <td>[Very, cute, -, I, do, n't, think, I, can, mak...</td>\n",
       "      <td>[Very, cute, -, I, don't, think, I, can, make,...</td>\n",
       "      <td>[Very, cute, -, I, don't, think, I, can, make,...</td>\n",
       "      <td>[Very, cute, -, I, don, ', t, think, I, can, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>dunno. Maybe the flu. I feel a bitbetter now.</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[dunno, ., Maybe, the, flu, ., I, feel, a, bit...</td>\n",
       "      <td>[dunno, ., Maybe, the, flu, ., I, feel, a, bit...</td>\n",
       "      <td>[dunno, ., Maybe, the, flu, ., I, feel, a, bit...</td>\n",
       "      <td>[dunno, ., Maybe, the, flu, ., I, feel, a, bit...</td>\n",
       "      <td>[dunno, ., Maybe, the, flu, ., I, feel, a, bit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11403</th>\n",
       "      <td>ya i did i seen all them but Robert</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[ya, i, did, i, seen, all, them, but, Robert]</td>\n",
       "      <td>[ya, i, did, i, seen, all, them, but, Robert]</td>\n",
       "      <td>[ya, i, did, i, seen, all, them, but, Robert]</td>\n",
       "      <td>[ya, i, did, i, seen, all, them, but, Robert]</td>\n",
       "      <td>[ya, i, did, i, seen, all, them, but, Robert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22643</th>\n",
       "      <td>I was the blue  lol http://twitpic.com/67zgz</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, was, the, blue, lol, http://twitpic.com/67...</td>\n",
       "      <td>[I, was, the, blue, lol, http, :, //twitpic.co...</td>\n",
       "      <td>[I, was, the, blue, lol, http://twitpic.com/67...</td>\n",
       "      <td>[I, was, the, blue, lol, http://twitpic.com/67...</td>\n",
       "      <td>[I, was, the, blue, lol, http, :, /, /, twitpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Waking up early to go to the gym</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[Waking, up, early, to, go, to, the, gym]</td>\n",
       "      <td>[Waking, up, early, to, go, to, the, gym]</td>\n",
       "      <td>[Waking, up, early, to, go, to, the, gym]</td>\n",
       "      <td>[Waking, up, early, to, go, to, the, gym]</td>\n",
       "      <td>[Waking, up, early, to, go, to, the, gym]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19474</th>\n",
       "      <td>it drained my energy</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[it, drained, my, energy]</td>\n",
       "      <td>[it, drained, my, energy]</td>\n",
       "      <td>[it, drained, my, energy]</td>\n",
       "      <td>[it, drained, my, energy]</td>\n",
       "      <td>[it, drained, my, energy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "1315    it was a biligual sweatshop LOL I talk 2 him ...  negative   \n",
       "6422        On the bus to NYC   http://yfrog.com/08kaifj   neutral   \n",
       "17769  : Ok its suppose 2b followfriday not unfollow ...  positive   \n",
       "16344  had such a fun time with allegra tonite!!! we ...  positive   \n",
       "15836   Very cute - I don't think I can make it to Ma...   neutral   \n",
       "18695      dunno. Maybe the flu. I feel a bitbetter now.  positive   \n",
       "11403                ya i did i seen all them but Robert   neutral   \n",
       "22643       I was the blue  lol http://twitpic.com/67zgz   neutral   \n",
       "1332                    Waking up early to go to the gym   neutral   \n",
       "19474                               it drained my energy  negative   \n",
       "\n",
       "       sentiment_num                                    Tweet Tokenizer  \\\n",
       "1315              -1  [it, was, a, biligual, sweatshop, LOL, I, talk...   \n",
       "6422               0  [On, the, bus, to, NYC, http://yfrog.com/08kaifj]   \n",
       "17769              1  [:, Ok, its, suppose, 2b, followfriday, not, u...   \n",
       "16344              1  [had, such, a, fun, time, with, allegra, tonit...   \n",
       "15836              0  [Very, cute, -, I, don't, think, I, can, make,...   \n",
       "18695              1  [dunno, ., Maybe, the, flu, ., I, feel, a, bit...   \n",
       "11403              0      [ya, i, did, i, seen, all, them, but, Robert]   \n",
       "22643              0  [I, was, the, blue, lol, http://twitpic.com/67...   \n",
       "1332               0          [Waking, up, early, to, go, to, the, gym]   \n",
       "19474             -1                          [it, drained, my, energy]   \n",
       "\n",
       "                                           Word Tokenize  \\\n",
       "1315   [it, was, a, biligual, sweatshop, LOL, I, talk...   \n",
       "6422   [On, the, bus, to, NYC, http, :, //yfrog.com/0...   \n",
       "17769  [:, Ok, its, suppose, 2b, followfriday, not, u...   \n",
       "16344  [had, such, a, fun, time, with, allegra, tonit...   \n",
       "15836  [Very, cute, -, I, do, n't, think, I, can, mak...   \n",
       "18695  [dunno, ., Maybe, the, flu, ., I, feel, a, bit...   \n",
       "11403      [ya, i, did, i, seen, all, them, but, Robert]   \n",
       "22643  [I, was, the, blue, lol, http, :, //twitpic.co...   \n",
       "1332           [Waking, up, early, to, go, to, the, gym]   \n",
       "19474                          [it, drained, my, energy]   \n",
       "\n",
       "                                         Casual Tokenize  \\\n",
       "1315   [it, was, a, biligual, sweatshop, LOL, I, talk...   \n",
       "6422   [On, the, bus, to, NYC, http://yfrog.com/08kaifj]   \n",
       "17769  [:, Ok, its, suppose, 2b, followfriday, not, u...   \n",
       "16344  [had, such, a, fun, time, with, allegra, tonit...   \n",
       "15836  [Very, cute, -, I, don't, think, I, can, make,...   \n",
       "18695  [dunno, ., Maybe, the, flu, ., I, feel, a, bit...   \n",
       "11403      [ya, i, did, i, seen, all, them, but, Robert]   \n",
       "22643  [I, was, the, blue, lol, http://twitpic.com/67...   \n",
       "1332           [Waking, up, early, to, go, to, the, gym]   \n",
       "19474                          [it, drained, my, energy]   \n",
       "\n",
       "                                               Twokenize  \\\n",
       "1315   [it, was, a, biligual, sweatshop, LOL, I, talk...   \n",
       "6422   [On, the, bus, to, NYC, http://yfrog.com/08kaifj]   \n",
       "17769  [:, Ok, its, suppose, 2b, followfriday, not, u...   \n",
       "16344  [had, such, a, fun, time, with, allegra, tonit...   \n",
       "15836  [Very, cute, -, I, don't, think, I, can, make,...   \n",
       "18695  [dunno, ., Maybe, the, flu, ., I, feel, a, bit...   \n",
       "11403      [ya, i, did, i, seen, all, them, but, Robert]   \n",
       "22643  [I, was, the, blue, lol, http://twitpic.com/67...   \n",
       "1332           [Waking, up, early, to, go, to, the, gym]   \n",
       "19474                          [it, drained, my, energy]   \n",
       "\n",
       "                                              Twikenizer  \n",
       "1315   [it, was, a, biligual, sweatshop, LOL, I, talk...  \n",
       "6422   [On, the, bus, to, NYC, http, :, /, /, yfrog, ...  \n",
       "17769  [:, Ok, its, suppose, 2b, followfriday, not, u...  \n",
       "16344  [had, such, a, fun, time, with, allegra, tonit...  \n",
       "15836  [Very, cute, -, I, don, ', t, think, I, can, m...  \n",
       "18695  [dunno, ., Maybe, the, flu, ., I, feel, a, bit...  \n",
       "11403      [ya, i, did, i, seen, all, them, but, Robert]  \n",
       "22643  [I, was, the, blue, lol, http, :, /, /, twitpi...  \n",
       "1332           [Waking, up, early, to, go, to, the, gym]  \n",
       "19474                          [it, drained, my, energy]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test different tokenizers\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "import twikenizer as twk\n",
    "import twokenize as tok\n",
    "\n",
    "dataframe['text'] = dataframe['text'].convert_dtypes(str)\n",
    "\n",
    "# the best\n",
    "tt = TweetTokenizer(reduce_len=True)\n",
    "dataframe['Tweet Tokenizer'] = dataframe['text'].apply(tt.tokenize)\n",
    "\n",
    "# very bad\n",
    "dataframe['Word Tokenize'] = dataframe['text'].apply(word_tokenize)\n",
    "\n",
    "# right in the middle\n",
    "dataframe['Casual Tokenize'] = dataframe['text'].apply(casual_tokenize)\n",
    "\n",
    "# very decent\n",
    "dataframe['Twokenize'] = dataframe['text'].apply(tok.tokenizeRawTweetText)\n",
    "\n",
    "# bad and very slow\n",
    "ti = twk.Twikenizer()\n",
    "ser = []\n",
    "for s in dataframe['text']:\n",
    "    ser.append(ti.tokenize(s))\n",
    "dataframe['Twikenizer'] = ser\n",
    "\n",
    "# Ranking:\n",
    "# 1. Tweet Tokenizer\n",
    "# 2. Twokenize\n",
    "# 3. Casual Tokenize\n",
    "# 4. Word Tokenize\n",
    "# 5. Twikenizer\n",
    "\n",
    "dataframe.sample(n=10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a940aff-4450-42d1-aae0-6386b59583c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "      <th>Tweet Tokenizer</th>\n",
       "      <th>Tweet Tokenizer Proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>it was a biligual sweatshop LOL I talk 2 him ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[it, was, a, biligual, sweatshop, LOL, I, talk...</td>\n",
       "      <td>[biligual, sweatshop, lol, talk, not, much, get]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6422</th>\n",
       "      <td>On the bus to NYC   http://yfrog.com/08kaifj</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[On, the, bus, to, NYC, http://yfrog.com/08kaifj]</td>\n",
       "      <td>[bus, nyc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17769</th>\n",
       "      <td>: Ok its suppose 2b followfriday not unfollow ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[:, Ok, its, suppose, 2b, followfriday, not, u...</td>\n",
       "      <td>[suppose, folowfriday, not, unfolow, friday, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16344</th>\n",
       "      <td>had such a fun time with allegra tonite!!! we ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[had, such, a, fun, time, with, allegra, tonit...</td>\n",
       "      <td>[fun, time, alegra, tonite, saw, good, movie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15836</th>\n",
       "      <td>Very cute - I don't think I can make it to Ma...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[Very, cute, -, I, don't, think, I, can, make,...</td>\n",
       "      <td>[cute, think, make, makerfaire, sadly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18695</th>\n",
       "      <td>dunno. Maybe the flu. I feel a bitbetter now.</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[dunno, ., Maybe, the, flu, ., I, feel, a, bit...</td>\n",
       "      <td>[duno, maybe, flu, feel, bitbeter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11403</th>\n",
       "      <td>ya i did i seen all them but Robert</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[ya, i, did, i, seen, all, them, but, Robert]</td>\n",
       "      <td>[see, robert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22643</th>\n",
       "      <td>I was the blue  lol http://twitpic.com/67zgz</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, was, the, blue, lol, http://twitpic.com/67...</td>\n",
       "      <td>[blue, lol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Waking up early to go to the gym</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>[Waking, up, early, to, go, to, the, gym]</td>\n",
       "      <td>[wake, early, gym]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19474</th>\n",
       "      <td>it drained my energy</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[it, drained, my, energy]</td>\n",
       "      <td>[drain, energy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "1315    it was a biligual sweatshop LOL I talk 2 him ...  negative   \n",
       "6422        On the bus to NYC   http://yfrog.com/08kaifj   neutral   \n",
       "17769  : Ok its suppose 2b followfriday not unfollow ...  positive   \n",
       "16344  had such a fun time with allegra tonite!!! we ...  positive   \n",
       "15836   Very cute - I don't think I can make it to Ma...   neutral   \n",
       "18695      dunno. Maybe the flu. I feel a bitbetter now.  positive   \n",
       "11403                ya i did i seen all them but Robert   neutral   \n",
       "22643       I was the blue  lol http://twitpic.com/67zgz   neutral   \n",
       "1332                    Waking up early to go to the gym   neutral   \n",
       "19474                               it drained my energy  negative   \n",
       "\n",
       "       sentiment_num                                    Tweet Tokenizer  \\\n",
       "1315              -1  [it, was, a, biligual, sweatshop, LOL, I, talk...   \n",
       "6422               0  [On, the, bus, to, NYC, http://yfrog.com/08kaifj]   \n",
       "17769              1  [:, Ok, its, suppose, 2b, followfriday, not, u...   \n",
       "16344              1  [had, such, a, fun, time, with, allegra, tonit...   \n",
       "15836              0  [Very, cute, -, I, don't, think, I, can, make,...   \n",
       "18695              1  [dunno, ., Maybe, the, flu, ., I, feel, a, bit...   \n",
       "11403              0      [ya, i, did, i, seen, all, them, but, Robert]   \n",
       "22643              0  [I, was, the, blue, lol, http://twitpic.com/67...   \n",
       "1332               0          [Waking, up, early, to, go, to, the, gym]   \n",
       "19474             -1                          [it, drained, my, energy]   \n",
       "\n",
       "                                    Tweet Tokenizer Proc  \n",
       "1315    [biligual, sweatshop, lol, talk, not, much, get]  \n",
       "6422                                          [bus, nyc]  \n",
       "17769  [suppose, folowfriday, not, unfolow, friday, w...  \n",
       "16344      [fun, time, alegra, tonite, saw, good, movie]  \n",
       "15836             [cute, think, make, makerfaire, sadly]  \n",
       "18695                 [duno, maybe, flu, feel, bitbeter]  \n",
       "11403                                      [see, robert]  \n",
       "22643                                        [blue, lol]  \n",
       "1332                                  [wake, early, gym]  \n",
       "19474                                    [drain, energy]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing: all lower case, remove stop words and punctuation (mostly), remove short words (less than 3), stemming, POS tag, lemmatization, remove repeated characters\n",
    "\n",
    "dataframe.drop(['Word Tokenize', 'Casual Tokenize', 'Twokenize', 'Twikenizer'], axis=1, inplace=True)\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "\n",
    "tag_dict = {'J': wn.ADJ,\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV}\n",
    "\n",
    "def lemmatize(sent):\n",
    "    lemmas = []\n",
    "    for word_tag in sent:\n",
    "        lemmas.append(lemmatizer.lemmatize(word_tag[0], tag_dict.get(word_tag[1][0], wn.NOUN)))\n",
    "    return lemmas\n",
    "\n",
    "def remove_repeated_characters(tokens):  #not the best, if it can't find the word, reduces correct words into incorrect\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')  # alphanumeric, repeated characters, rest of the word\n",
    "    match_substitution = r'\\1\\2\\3'  # 1, 2 or 3 repeated characters\n",
    "    def replace(old_word):\n",
    "        if wn.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "def remove_digits(token):\n",
    "    pattern = r'[0-9]'\n",
    "    new_token = re.sub(pattern, '', token)\n",
    "    return new_token\n",
    "\n",
    "# nltk.download()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "stop_words += ['dont', 'cant', 'thats', 'didnt', 'wont', 'ive', 'whats', 'havent', 'ima', 'aint', 'canot', 'isnt', 'shes', 'hes', 'yal'] \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = dataframe['Tweet Tokenizer'].values\n",
    "tokens = [[word.lower() for word in sentence if word not in string.punctuation] for sentence in tokens]\n",
    "tokens = [[word for word in sentence if '.' not in word and not word.startswith(('@', '#', '_'))] for sentence in tokens]\n",
    "tokens = [[remove_digits(word) for word in sentence] for sentence in tokens]\n",
    "tokens = [remove_repeated_characters(sentence) for sentence in tokens]\n",
    "tokens = [lemmatize(pos_tag(token)) for token in tokens]\n",
    "tokens = [[word.split(\"'\")[0] for word in sentence] for sentence in tokens]\n",
    "tokens = [[word for word in sentence if len(word)>=3 or len(word)>=45 or word=='no'] for sentence in tokens]\n",
    "tokens = [[word for word in sentence if word not in stop_words and word!='...'] for sentence in tokens]\n",
    "\n",
    "dataframe['Tweet Tokenizer Proc'] = tokens\n",
    "\n",
    "dataframe.sample(n=10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9085504-8f6c-4ef8-a9b1-77c3b8011f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('tokenized_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bdc50a-f4bd-4a17-8c13-d3004eeb01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('tokenized_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e30e7e39-0d08-4276-8123-6cda52935d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 3011),\n",
       " ('day', 2484),\n",
       " ('not', 1875),\n",
       " ('good', 1829),\n",
       " ('work', 1524),\n",
       " ('no', 1464),\n",
       " ('like', 1440),\n",
       " ('love', 1386),\n",
       " ('today', 1167),\n",
       " ('time', 1098),\n",
       " ('one', 1073),\n",
       " ('know', 1054),\n",
       " ('lol', 1026),\n",
       " ('think', 1021),\n",
       " ('see', 1013),\n",
       " ('happy', 1008),\n",
       " ('want', 987),\n",
       " ('make', 971),\n",
       " ('miss', 962),\n",
       " ('really', 921),\n",
       " ('back', 920),\n",
       " ('well', 909),\n",
       " ('night', 809),\n",
       " ('feel', 792),\n",
       " ('mother', 791)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the most frequent words\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "all_words = dataframe['Tweet Tokenizer Proc'].to_numpy()\n",
    "all_words = np.concatenate(all_words).ravel().tolist()\n",
    "\n",
    "fdist = FreqDist(all_words)\n",
    "fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f20907-e264-491c-a11b-8727cb1d3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dataframe['Tweet Tokenizer Proc'].astype(str)\n",
    "\n",
    "# Bag of Words, BoN = ngram_range=(1,3) uni OK, bi 28.9GiB, tri 55.8GiB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(lowercase=False)\n",
    "\n",
    "bow = count_vect.fit_transform(tokens).toarray()\n",
    "\n",
    "# TF-IDF, ngram_range=(1, 3), max_features=10000\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(min_df=2, ngram_range=(1, 3), strip_accents='unicode', norm='l2')\n",
    "\n",
    "tfidf = tfidf_vect.fit_transform(tokens).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "571a150e-2314-4015-aa52-0203e0ae6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW - Number of sentences: 27480, Unique words: 17964 \n",
      "Example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bylaurenluke', 2260),\n",
       " ('superpower', 15216),\n",
       " ('brings', 1995),\n",
       " ('lexi', 9026),\n",
       " ('krathong', 8695),\n",
       " ('enlgland', 4970),\n",
       " ('muzik', 10467),\n",
       " ('nba', 10605),\n",
       " ('cade', 2279),\n",
       " ('shelter', 13950)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "res = random.choices(list(count_vect.vocabulary_.items()), k=10)\n",
    "print(f'BoW - Number of sentences: {bow.shape[0]}, Unique words: {bow.shape[1]} \\nExample:')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "efe00274-75c7-4cb5-82d2-d830e8d4c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfIdf - Number of sentences: 27480, Unique words and ngrams: 23658 \n",
      "Example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('get hang', 9.42949066654802),\n",
       " ('bikini', 10.122637847107965),\n",
       " ('goto', 9.834955774656184),\n",
       " ('small', 7.414587646005755),\n",
       " ('hahahaha', 8.043196305428129),\n",
       " ('thought would', 10.122637847107965),\n",
       " ('shape', 9.024025558439854),\n",
       " ('school day', 9.42949066654802),\n",
       " ('sunshine gona', 10.122637847107965),\n",
       " ('ant', 9.42949066654802)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'TfIdf - Number of sentences: {tfidf.shape[0]}, Unique words and ngrams: {tfidf.shape[1]} \\nExample:')\n",
    "\n",
    "nums = random.choices(range(tfidf.shape[1]), k=10)\n",
    "res = [(tfidf_vect.get_feature_names_out()[i], tfidf_vect.idf_[i]) for i in nums]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a360b72-13ea-4599-85d8-77c2810ad8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Dummy BoW   0.32787481804949054\n",
      "Base Dummy TdIdf 0.32787481804949054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def base_classifier(rep, test=0.2):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rep, dataframe['sentiment_num'], test_size=test, random_state=7)\n",
    "    dummy = DummyClassifier(strategy='uniform', random_state=7)\n",
    "    dummy.fit(x_train, y_train)\n",
    "    return dummy.score(x_test, y_test)\n",
    "\n",
    "print(f'Base Dummy BoW   {base_classifier(bow)}')\n",
    "print(f'Base Dummy TdIdf {base_classifier(tfidf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4400a9d6-1d9f-44ca-80c4-50dec14a31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    \n",
    "def ml_cross(classifier, rep, test=0.2):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rep, dataframe['sentiment_num'], test_size=test, random_state=7)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    cv_scores = cross_val_score(classifier, x_train, y_train, cv=5, n_jobs=-1)\n",
    "    cv_mean_score = np.mean(cv_scores)\n",
    "    test_score = classifier.score(x_test, y_test)\n",
    "    return cv_scores, cv_mean_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe5ac3c7-db78-4d55-81e4-e25517ce6e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes scores BoW: [0.65476461 0.64225608 0.6472595  0.65863088 0.65468608] \n",
      "Mean score: 0.6515194307972825 \n",
      "Test score: 0.6550218340611353\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "temp, mnb_bow_cv_mean_score, mnb_bow_test_score = ml_cross(mnb, bow)\n",
    "print(f'Multinomial Naive Bayes scores BoW: {temp} \\nMean score: {mnb_bow_cv_mean_score} \\nTest score: {mnb_bow_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103006db-fc96-4ff0-a08b-fb8aea33a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes scores TfIdf: [0.63133955 0.62952013 0.62065044 0.63179441 0.63717015] \n",
      "Mean score: 0.6300949360998265 \n",
      "Test score: 0.6381004366812227\n"
     ]
    }
   ],
   "source": [
    "temp, mnb_tfidf_cv_mean_score, mnb_tfidf_test_score = ml_cross(mnb, tfidf)\n",
    "print(f'Multinomial Naive Bayes scores TfIdf: {temp} \\nMean score: {mnb_tfidf_cv_mean_score} \\nTest score: {mnb_tfidf_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d1a528-94c7-48e6-b27d-3a12ee690162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitc Regression scores BoW: [0.68796907 0.6906982  0.69092563        nan 0.70200182] \n",
      "Mean score: nan \n",
      "Test score: 0.6925036390101892\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter=10000, C=1, random_state=7)\n",
    "temp, lr_bow_cv_mean_score, lr_bow_test_score = ml_cross(lr, bow)\n",
    "print(f'Logisitc Regression scores BoW: {temp} \\nMean score: {lr_bow_cv_mean_score} \\nTest score: {lr_bow_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a421567-9f6b-49b5-856e-111d025a65a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitc Regression scores TfIdf: [0.68887878 0.69115306 0.68592222 0.69752104 0.69131028] \n",
      "Mean score: 0.6909570757462851 \n",
      "Test score: 0.6932314410480349\n"
     ]
    }
   ],
   "source": [
    "temp, lr_tfidf_cv_mean_score, lr_tfidf_test_score = ml_cross(lr, tfidf)\n",
    "print(f'Logisitc Regression scores TfIdf: {temp} \\nMean score: {lr_tfidf_cv_mean_score} \\nTest score: {lr_tfidf_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46884b25-4a31-450f-a5f8-2f2cc1b01ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM scores BoW: [0.66340687 0.666136   0.66704571 0.67295884 0.67470428] \n",
      "Mean score: 0.6688503390619338 \n",
      "Test score: 0.6663027656477438\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', max_iter=10000, C=1, random_state=7)\n",
    "temp, svm_bow_cv_mean_score, svm_bow_test_score = ml_cross(svm, bow)\n",
    "print(f'Linear SVM scores BoW: {temp} \\nMean score: {svm_bow_cv_mean_score} \\nTest score: {svm_bow_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d7c8a0f-3d42-4233-bf3b-a249fe295411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM scores TfIdf: [0.67068456 0.68046395 0.66499886 0.67455083 0.67993631] \n",
      "Mean score: 0.6741269018105859 \n",
      "Test score: 0.6750363901018923\n"
     ]
    }
   ],
   "source": [
    "temp, svm_tfidf_cv_mean_score, svm_tfidf_test_score = ml_cross(svm, tfidf)\n",
    "print(f'Linear SVM scores TfIdf: {temp} \\nMean score: {svm_tfidf_cv_mean_score} \\nTest score: {svm_tfidf_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6458baac-4593-470f-a0de-629b84d74cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (SGD) scores BoW: [0.69865818 0.69820332 0.69797589        nan        nan] \n",
      "Mean score: nan \n",
      "Test score: 0.7083333333333334\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=10000, random_state=7)\n",
    "temp, svmsgd_bow_cv_mean_score, svmsgd_bow_test_score = ml_cross(sgd, bow)\n",
    "print(f'Linear SVM (SGD) scores BoW: {temp} \\nMean score: {svmsgd_bow_cv_mean_score} \\nTest score: {svmsgd_bow_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f831b99-9871-402a-882d-e8727551e2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM (SGD) scores TfIdf: [0.69752104 0.69843075 0.69456448 0.70798272 0.69267516] \n",
      "Mean score: 0.6982348271621213 \n",
      "Test score: 0.7079694323144105\n"
     ]
    }
   ],
   "source": [
    "temp, svmsgd_tfidf_cv_mean_score, svmsgd_tfidf_test_score = ml_cross(sgd, tfidf)\n",
    "print(f'Linear SVM (SGD) scores TfIdf: {temp} \\nMean score: {svmsgd_tfidf_cv_mean_score} \\nTest score: {svmsgd_tfidf_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601877ef-5702-421c-9652-fcf4318eca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest scores BoW: [0.6784171  0.70138731 0.69774846 0.68796907 0.69517743] \n",
      "Mean score: 0.6921398761625668 \n",
      "Test score: 0.7003275109170306\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20, random_state=7)\n",
    "temp, rfc_bow_cv_mean_score, rfc_bow_test_score = ml_cross(rfc, bow)\n",
    "print(f'Random Forest scores BoW: {temp} \\nMean score: {rfc_bow_cv_mean_score} \\nTest score: {rfc_bow_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186125a8-b563-43e0-add7-5236daf79b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest scores TfIdf: [0.6881965  0.70025017 0.69274505 0.69115306 0.69494995] \n",
      "Mean score: 0.6934589470072552 \n",
      "Test score: 0.7030567685589519\n"
     ]
    }
   ],
   "source": [
    "tfidf_small = np.float32(tfidf)\n",
    "rfc = RandomForestClassifier(n_estimators=20, random_state=7)\n",
    "temp, rfc_tfidf_cv_mean_score, rfc_tfidf_test_score = ml_cross(rfc, tfidf_small)\n",
    "print(f'Random Forest scores TfIdf: {temp} \\nMean score: {rfc_tfidf_cv_mean_score} \\nTest score: {rfc_tfidf_test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de56f5-59eb-417c-a571-be768ae466ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_ml = pd.DataFrame([['Naive Bayes', mnb_bow_cv_mean_score, mnb_bow_test_score, mnb_tfidf_cv_mean_score, mnb_tfidf_test_score],\n",
    "#                         ['Logistic Regression', lr_bow_cv_mean_score, lr_bow_test_score, lr_tfidf_cv_mean_score, lr_tfidf_test_score],\n",
    "#                         ['Linear SVM', svm_bow_cv_mean_score, svm_bow_test_score, svm_tfidf_cv_mean_score, svm_tfidf_test_score],\n",
    "#                         ['Linear SVM (SGD)', svmsgd_bow_cv_mean_score, svmsgd_bow_test_score, svmsgd_tfidf_cv_mean_score, svmsgd_tfidf_test_score],\n",
    "#                         ['Random Forest', rfc_bow_cv_mean_score, rfc_bow_test_score, rfc_tfidf_cv_mean_score, rfc_tfidf_test_score]],\n",
    "#                        columns=['Model', 'CV Score (BoW)', 'Test Score (BoW)', 'CV Score (TF-IDF)', 'Test Score (TF-IDF)']).T\n",
    "# results_ml.to_csv('results_ml.csv')\n",
    "# results_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a471672-4559-43a5-b5a0-c711fd67c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Word2Vec word embedding; Continuous Bag of Words and Skip-Gram\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "tokens = dataframe['Tweet Tokenizer Proc']\n",
    "#model.wv.key_to_index #dict with embeddings\n",
    "#model.wv.get_normed_vectors() # all normalized vectors\n",
    "\n",
    "w2v_cbow = Word2Vec(sentences=tokens, window=2, min_count=1, sg=0, cbow_mean=0, workers=6)\n",
    "w2v_cbow_tokens = [[w2v_cbow.wv[word].tolist() for word in sentence] for sentence in tokens]\n",
    "w2v_cbow_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=100) for word in w2v_cbow_tokens]\n",
    "\n",
    "w2v_sg = Word2Vec(sentences=tokens, window=2, min_count=1, sg=1, workers=6) \n",
    "w2v_sg_tokens = [[w2v_sg.wv[word].tolist() for word in sentence] for sentence in tokens]\n",
    "w2v_sg_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=100) for word in w2v_sg_tokens]\n",
    "\n",
    "ft_cbow = FastText(tokens, window=2, min_count=1, sg=0, workers=6)\n",
    "ft_cbow_tokens = [[ft_cbow.wv[word].tolist() for word in sentence] for sentence in tokens]\n",
    "ft_cbow_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=100) for word in ft_cbow_tokens]\n",
    "\n",
    "ft_sg = FastText(tokens, window=2, min_count=1, sg=1, workers=6)\n",
    "ft_sg_tokens = [[ft_sg.wv[word].tolist() for word in sentence] for sentence in tokens]\n",
    "ft_sg_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=100) for word in ft_sg_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15ada4ce-ba79-4b04-bf88-c5e7b5f2ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Doc2Vec, word embedding on a document (in this case sentence)\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def gensim_dbow():\n",
    "    tokens = dataframe['Tweet Tokenizer Proc'].to_numpy()\n",
    "    tagged_data = [TaggedDocument(words=tokens[i], tags=[str(i)]) for i, _d in enumerate(tokens)]\n",
    "    model = Doc2Vec(vector_size=100, min_count=1, epochs=10, workers=12)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=10)\n",
    "    return model\n",
    "\n",
    "dbow = gensim_dbow()\n",
    "dbow_tokens = [[dbow.wv[word].tolist() for word in sentence] for sentence in tokens]\n",
    "dbow_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=100) for word in dbow_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f5134f1-f456-4da7-b552-beb99e1f3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word vectors\n",
    "\n",
    "def avg_word_vecs(words, model, vocab, num_feat):\n",
    "    vecs = np.zeros((num_feat,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            nwords = nwords + 1.\n",
    "            vecs = np.add(vecs, model.wv[word])\n",
    "    if nwords:\n",
    "        vecs = np.divide(vecs, nwords)\n",
    "    return vecs\n",
    "\n",
    "def avg_word_vect(corp, model, num_feat):\n",
    "    vocab = set(model.wv.index_to_key)\n",
    "    feat = [avg_word_vecs(token, model, vocab, num_feat)\n",
    "                for token in corp]\n",
    "    return np.array(feat)\n",
    "\n",
    "w2v_model = Word2Vec(tokens, vector_size=100, window=100, min_count=2, sample=1e-3, sg=1, epochs=5, workers=10)\n",
    "avg_vecs = avg_word_vect(corp=tokens, model=w2v_model, num_feat=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64639f36-ab1a-43c0-aead-3af569a1b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove and spacy pretrained\n",
    "\n",
    "import spacy\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open('glove.twitter.27B.100d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = [float(item) for item in values[1:]]\n",
    "        embeddings_dict[word] = vector\n",
    "    \n",
    "glove_tokens = [[embeddings_dict[word] if word in embeddings_dict else np.random.uniform(low=-0.2, high=0.2, size=100) for word in sentence] for sentence in tokens]\n",
    "glove_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=100) for word in glove_tokens]\n",
    "\n",
    "sp = spacy.load('en_core_web_lg')\n",
    "spacy_tokens = [[sp.vocab.get_vector(word) if sp.vocab.has_vector else np.random.uniform(low=-0.2, high=0.2, size=300) for word in sentence] for sentence in tokens]\n",
    "spacy_tokens = [np.add.reduce(word) if word != [] else np.random.uniform(low=-0.2, high=0.2, size=300) for word in spacy_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e196dd0c-7af4-48df-a4b7-3a5f50a7b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes scores Word2Vec CBoW:      [0.42915624 0.41960428 0.4296111  0.42574483 0.42902639]\n",
      "Naive Bayes scores Word2Vec Skip-Gram: [0.43256766 0.42369798 0.4321128  0.42324312 0.43198362]\n",
      "Naive Bayes scores FastText CBoW:      [0.42051399 0.40641346 0.42028656 0.41869456 0.42151956]\n",
      "Naive Bayes scores FastText Skip-Gram: [0.43302252 0.41482829 0.42824653 0.41755743 0.42902639]\n",
      "Naive Bayes scores Doc2Vec:            [0.44393905 0.43438708 0.43870821 0.43484194 0.44972702]\n",
      "Naive Bayes scores Avg. Vectors:       [0.47304981 0.4746418  0.4664544  0.47327723 0.47520473]\n",
      "Naive Bayes scores Glove:              [0.41642029 0.41687514 0.415738   0.41346373 0.41537762]\n",
      "Naive Bayes scores Spacy:              [0.44575847 0.44211963 0.44712304 0.44575847 0.44836215]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale because NB only accepts positive values\n",
    "w2v_cbow_tokens_nb = MinMaxScaler().fit_transform(w2v_cbow_tokens)\n",
    "w2v_sg_tokens_nb = MinMaxScaler().fit_transform(w2v_sg_tokens)\n",
    "ft_cbow_tokens_nb = MinMaxScaler().fit_transform(ft_cbow_tokens)\n",
    "ft_sg_tokens_nb = MinMaxScaler().fit_transform(ft_sg_tokens)\n",
    "dbow_tokens_nb = MinMaxScaler().fit_transform(dbow_tokens)\n",
    "avg_vecs_nb = MinMaxScaler().fit_transform(avg_vecs)\n",
    "glove_tokens_nb = MinMaxScaler().fit_transform(glove_tokens)\n",
    "spacy_tokens_nb = MinMaxScaler().fit_transform(spacy_tokens)\n",
    "\n",
    "temp, w2v_cbow_ms_mnb, w2v_cbow_ts_mnb = ml_cross(mnb, w2v_cbow_tokens_nb)\n",
    "print(f'Naive Bayes scores Word2Vec CBoW:      {temp}')\n",
    "temp, w2v_sg_ms_mnb, w2v_sg_ts_mnb = ml_cross(mnb, w2v_sg_tokens_nb)\n",
    "print(f'Naive Bayes scores Word2Vec Skip-Gram: {temp}')\n",
    "\n",
    "temp, ft_cbow_ms_mnb, ft_cbow_ts_mnb = ml_cross(mnb, ft_cbow_tokens_nb)\n",
    "print(f'Naive Bayes scores FastText CBoW:      {temp}')\n",
    "temp, ft_sg_ms_mnb, ft_sg_ts_mnb = ml_cross(mnb, ft_sg_tokens_nb)\n",
    "print(f'Naive Bayes scores FastText Skip-Gram: {temp}')\n",
    "\n",
    "temp, dbow_ms_mnb, dbow_ts_mnb = ml_cross(mnb, dbow_tokens_nb)\n",
    "print(f'Naive Bayes scores Doc2Vec:            {temp}')\n",
    "temp, avg_vec_ms_mnb, avg_vec_ts_mnb = ml_cross(mnb, avg_vecs_nb)\n",
    "print(f'Naive Bayes scores Avg. Vectors:       {temp}')\n",
    "\n",
    "temp, glove_ms_mnb, glove_ts_mnb = ml_cross(mnb, glove_tokens_nb)\n",
    "print(f'Naive Bayes scores Glove:              {temp}')\n",
    "temp, spacy_ms_mnb, spacy_ts_mnb = ml_cross(mnb, spacy_tokens_nb)\n",
    "print(f'Naive Bayes scores Spacy:              {temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82cd9938-a205-4e1f-b43a-8fea5c114a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression scores Word2Vec CBoW:      [0.60814191 0.60495793 0.60632249 0.60359336 0.60850773]\n",
      "Logistic Regression scores Word2Vec Skip-Gram: [0.62178758 0.63202183 0.62451672 0.62337958 0.62420382]\n",
      "Logistic Regression scores FastText CBoW:      [0.58107801 0.58357971 0.59040255 0.56925176 0.5878071 ]\n",
      "Logistic Regression scores FastText Skip-Gram: [0.61269047 0.62178758 0.60996134 0.61769388 0.61260237]\n",
      "Logistic Regression scores Doc2Vec:            [0.63042984 0.62883784 0.63679782 0.63588811 0.63466788]\n",
      "Logistic Regression scores Avg. Vectors:       [0.62701842 0.61928588 0.62087787 0.61746645 0.61919927]\n",
      "Logistic Regression scores Glove:              [0.66954742 0.65180805 0.6554469  0.65362747 0.65991811]\n",
      "Logistic Regression scores Spacy:              [0.6743234  0.66931999 0.6800091  0.67318626 0.67470428]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter=10000, C=1, random_state=7)\n",
    "\n",
    "temp, w2v_cbow_ms_lr, w2v_cbow_ts_lr = ml_cross(lr, w2v_cbow_tokens)\n",
    "print(f'Logistic Regression scores Word2Vec CBoW:      {temp}')\n",
    "temp, w2v_sg_ms_lr, w2v_sg_ts_lr = ml_cross(lr, w2v_sg_tokens)\n",
    "print(f'Logistic Regression scores Word2Vec Skip-Gram: {temp}')\n",
    "\n",
    "temp, ft_cbow_ms_lr, ft_cbow_ts_lr = ml_cross(lr, ft_cbow_tokens)\n",
    "print(f'Logistic Regression scores FastText CBoW:      {temp}')\n",
    "temp, ft_sg_ms_lr, ft_sg_ts_lr = ml_cross(lr, ft_sg_tokens)\n",
    "print(f'Logistic Regression scores FastText Skip-Gram: {temp}')\n",
    "\n",
    "temp, dbow_ms_lr, dbow_ts_lr = ml_cross(lr, dbow_tokens)\n",
    "print(f'Logistic Regression scores Doc2Vec:            {temp}')\n",
    "temp, avg_vec_ms_lr, avg_vec_ts_lr = ml_cross(lr, avg_vecs)\n",
    "print(f'Logistic Regression scores Avg. Vectors:       {temp}')\n",
    "\n",
    "temp, glove_ms_lr, glove_ts_lr = ml_cross(lr, glove_tokens)\n",
    "print(f'Logistic Regression scores Glove:              {temp}')\n",
    "temp, spacy_ms_lr, spacy_ts_lr = ml_cross(lr, spacy_tokens)\n",
    "print(f'Logistic Regression scores Spacy:              {temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41e1f247-3231-46a8-96bb-c822edc55246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines scores Word2Vec CBoW:      [0.61428247 0.61109848 0.60836934 0.60382079 0.60873521]\n",
      "Support Vector Machines scores Word2Vec Skip-Gram: [0.62337958 0.62883784 0.625199   0.62724585 0.62829845]\n",
      "Support Vector Machines scores FastText CBoW:      [0.60200136 0.60950648 0.5990448  0.5867637  0.60395814]\n",
      "Support Vector Machines scores FastText Skip-Gram: [0.6211053  0.62474414 0.61496475 0.62428929 0.62215651]\n",
      "Support Vector Machines scores Doc2Vec:            [0.63315897 0.62770071 0.63247669 0.63861724 0.63193813]\n",
      "Support Vector Machines scores Avg. Vectors:       [0.63384126 0.62474414 0.63293154 0.62997498 0.62420382]\n",
      "Support Vector Machines scores Glove:              [0.66431658 0.65590175 0.66113259 0.65499204 0.66264786]\n",
      "Support Vector Machines scores Spacy:              [0.68182852 0.67500569 0.68091881 0.67932681 0.67879891]\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', max_iter=10000, C=1, random_state=7, dual=False)\n",
    "\n",
    "temp, w2v_cbow_ms_svm, w2v_cbow_ts_svm = ml_cross(svm, w2v_cbow_tokens)\n",
    "print(f'Support Vector Machines scores Word2Vec CBoW:      {temp}')\n",
    "temp, w2v_sg_ms_svm, w2v_sg_ts_svm = ml_cross(svm, w2v_sg_tokens)\n",
    "print(f'Support Vector Machines scores Word2Vec Skip-Gram: {temp}')\n",
    "\n",
    "temp, ft_cbow_ms_svm, ft_cbow_ts_svm = ml_cross(svm, ft_cbow_tokens)\n",
    "print(f'Support Vector Machines scores FastText CBoW:      {temp}')\n",
    "temp, ft_sg_ms_svm, ft_sg_ts_svm = ml_cross(svm, ft_sg_tokens)\n",
    "print(f'Support Vector Machines scores FastText Skip-Gram: {temp}')\n",
    "\n",
    "temp, dbow_ms_svm, dbow_ts_svm = ml_cross(svm, dbow_tokens)\n",
    "print(f'Support Vector Machines scores Doc2Vec:            {temp}')\n",
    "temp, avg_vec_ms_svm, avg_vec_ts_svm = ml_cross(svm, avg_vecs)\n",
    "print(f'Support Vector Machines scores Avg. Vectors:       {temp}')\n",
    "\n",
    "temp, glove_ms_svm, glove_ts_svm = ml_cross(svm, glove_tokens)\n",
    "print(f'Support Vector Machines scores Glove:              {temp}')\n",
    "temp, spacy_ms_svm, spacy_ts_svm = ml_cross(svm, spacy_tokens)\n",
    "print(f'Support Vector Machines scores Spacy:              {temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c97c80a-3bca-4d3f-bac1-484c131807c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stohastic Gradient Descent (SVM) scores Word2Vec CBoW:      [0.55628838 0.59153969 0.60382079 0.51012054 0.58257507]\n",
      "Stohastic Gradient Descent (SVM) scores Word2Vec Skip-Gram: [0.54241528 0.51648851 0.61678417 0.57630202 0.56619654]\n",
      "Stohastic Gradient Descent (SVM) scores FastText CBoW:      [0.48987946 0.49556516 0.51284967 0.37411872 0.51819836]\n",
      "Stohastic Gradient Descent (SVM) scores FastText Skip-Gram: [0.48669547 0.58585399 0.55310439 0.58585399 0.54299363]\n",
      "Stohastic Gradient Descent (SVM) scores Doc2Vec:            [0.53013418 0.59472368 0.57175347 0.59153969 0.5843949 ]\n",
      "Stohastic Gradient Descent (SVM) scores Avg. Vectors:       [0.61746645 0.61928588 0.61974073 0.62383443 0.616697  ]\n",
      "Stohastic Gradient Descent (SVM) scores Glove:              [0.58517171 0.63452354 0.60859677 0.54650898 0.56574158]\n",
      "Stohastic Gradient Descent (SVM) scores Spacy:              [0.63361383 0.58289743 0.58744599 0.59290425 0.60191083]\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=10000, random_state=7)\n",
    "\n",
    "temp, w2v_cbow_ms_sgd, w2v_cbow_ts_sgd = ml_cross(sgd, w2v_cbow_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores Word2Vec CBoW:      {temp}')\n",
    "temp, w2v_sg_ms_sgd, w2v_sg_ts_sgd = ml_cross(sgd, w2v_sg_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores Word2Vec Skip-Gram: {temp}')\n",
    "\n",
    "temp, ft_cbow_ms_sgd, ft_cbow_ts_sgd = ml_cross(sgd, ft_cbow_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores FastText CBoW:      {temp}')\n",
    "temp, ft_sg_ms_sgd, ft_sg_ts_sgd = ml_cross(sgd, ft_sg_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores FastText Skip-Gram: {temp}')\n",
    "\n",
    "temp, dbow_ms_sgd, dbow_ts_sgd = ml_cross(sgd, dbow_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores Doc2Vec:            {temp}')\n",
    "temp, avg_vec_ms_sgd, avg_vec_ts_sgd = ml_cross(sgd, avg_vecs)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores Avg. Vectors:       {temp}')\n",
    "\n",
    "temp, glove_ms_sgd, glove_ts_sgd = ml_cross(sgd, glove_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores Glove:              {temp}')\n",
    "temp, spacy_ms_sgd, spacy_ts_sgd = ml_cross(sgd, spacy_tokens)\n",
    "print(f'Stohastic Gradient Descent (SVM) scores Spacy:              {temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb1dcb5b-421f-4f96-bd85-147d15da94bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest scores Word2Vec CBoW:      [0.55060268 0.55378667 0.54946554 0.55424153 0.56619654]\n",
      "Random Forest scores Word2Vec Skip-Gram: [0.56584035 0.57539231 0.5801683  0.57266318 0.5843949 ]\n",
      "Random Forest scores FastText CBoW:      [0.49965886 0.48987946 0.49897657 0.51057539 0.50523203]\n",
      "Random Forest scores FastText Skip-Gram: [0.54628156 0.54628156 0.55446896 0.55310439 0.56119199]\n",
      "Random Forest scores Doc2Vec:            [0.58630885 0.59290425 0.59108483 0.58267    0.59190173]\n",
      "Random Forest scores Avg. Vectors:       [0.63088469 0.62087787 0.62883784 0.62224244 0.62261146]\n",
      "Random Forest scores Glove:              [0.63270412 0.623607   0.62337958 0.62406186 0.64058235]\n",
      "Random Forest scores Spacy:              [0.62019559 0.62952013 0.61564703 0.60859677 0.63034577]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=7)\n",
    "\n",
    "temp, w2v_cbow_ms_rfc, w2v_cbow_ts_rfc = ml_cross(rfc, w2v_cbow_tokens)\n",
    "print(f'Random Forest scores Word2Vec CBoW:      {temp}')\n",
    "temp, w2v_sg_ms_rfc, w2v_sg_ts_rfc = ml_cross(rfc, w2v_sg_tokens)\n",
    "print(f'Random Forest scores Word2Vec Skip-Gram: {temp}')\n",
    "\n",
    "temp, ft_cbow_ms_rfc, ft_cbow_ts_rfc = ml_cross(rfc, ft_cbow_tokens)\n",
    "print(f'Random Forest scores FastText CBoW:      {temp}')\n",
    "temp, ft_sg_ms_rfc, ft_sg_ts_rfc = ml_cross(rfc, ft_sg_tokens)\n",
    "print(f'Random Forest scores FastText Skip-Gram: {temp}')\n",
    "\n",
    "temp, dbow_ms_rfc, dbow_ts_rfc = ml_cross(rfc, dbow_tokens)\n",
    "print(f'Random Forest scores Doc2Vec:            {temp}')\n",
    "temp, avg_vec_ms_rfc, avg_vec_ts_rfc = ml_cross(rfc, avg_vecs)\n",
    "print(f'Random Forest scores Avg. Vectors:       {temp}')\n",
    "\n",
    "temp, glove_ms_rfc, glove_ts_rfc = ml_cross(rfc, glove_tokens)\n",
    "print(f'Random Forest scores Glove:              {temp}')\n",
    "temp, spacy_ms_rfc, spacy_ts_rfc = ml_cross(rfc, spacy_tokens)\n",
    "print(f'Random Forest scores Spacy:              {temp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e473b63-2a10-4d5c-bd4d-031a3ac82e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Representation</th>\n",
       "      <th>MultinomoalNB MS</th>\n",
       "      <th>MultinomoalNB TS</th>\n",
       "      <th>Logistic Regression MS</th>\n",
       "      <th>Logistic Regression TS</th>\n",
       "      <th>Support Vector Machines MS</th>\n",
       "      <th>Support Vector Machines TS</th>\n",
       "      <th>Stohastic Gradient Descent (SVM) MS</th>\n",
       "      <th>Stohastic Gradient Descent (SVM) TS</th>\n",
       "      <th>Random Forest MS</th>\n",
       "      <th>Random Forest TS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word2Vec CBoW</td>\n",
       "      <td>0.426629</td>\n",
       "      <td>0.433406</td>\n",
       "      <td>0.606305</td>\n",
       "      <td>0.619905</td>\n",
       "      <td>0.609261</td>\n",
       "      <td>0.618086</td>\n",
       "      <td>0.568869</td>\n",
       "      <td>0.547307</td>\n",
       "      <td>0.554859</td>\n",
       "      <td>0.569505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word2Vec Skip-Gram</td>\n",
       "      <td>0.428721</td>\n",
       "      <td>0.440138</td>\n",
       "      <td>0.625182</td>\n",
       "      <td>0.627729</td>\n",
       "      <td>0.626592</td>\n",
       "      <td>0.632278</td>\n",
       "      <td>0.563637</td>\n",
       "      <td>0.555495</td>\n",
       "      <td>0.575692</td>\n",
       "      <td>0.586426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FastText CBoW</td>\n",
       "      <td>0.417486</td>\n",
       "      <td>0.425582</td>\n",
       "      <td>0.582424</td>\n",
       "      <td>0.594978</td>\n",
       "      <td>0.600255</td>\n",
       "      <td>0.614447</td>\n",
       "      <td>0.478122</td>\n",
       "      <td>0.547489</td>\n",
       "      <td>0.500864</td>\n",
       "      <td>0.505822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FastText Skip-Gram</td>\n",
       "      <td>0.424536</td>\n",
       "      <td>0.435953</td>\n",
       "      <td>0.614947</td>\n",
       "      <td>0.621179</td>\n",
       "      <td>0.621452</td>\n",
       "      <td>0.629367</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>0.582424</td>\n",
       "      <td>0.552266</td>\n",
       "      <td>0.558042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.440321</td>\n",
       "      <td>0.447780</td>\n",
       "      <td>0.633324</td>\n",
       "      <td>0.638464</td>\n",
       "      <td>0.632778</td>\n",
       "      <td>0.639374</td>\n",
       "      <td>0.574509</td>\n",
       "      <td>0.595706</td>\n",
       "      <td>0.588974</td>\n",
       "      <td>0.596070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Avg. Vector</td>\n",
       "      <td>0.472526</td>\n",
       "      <td>0.486536</td>\n",
       "      <td>0.620770</td>\n",
       "      <td>0.620451</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>0.628457</td>\n",
       "      <td>0.619405</td>\n",
       "      <td>0.620997</td>\n",
       "      <td>0.625091</td>\n",
       "      <td>0.626092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Glove</td>\n",
       "      <td>0.415575</td>\n",
       "      <td>0.431405</td>\n",
       "      <td>0.658070</td>\n",
       "      <td>0.661936</td>\n",
       "      <td>0.659798</td>\n",
       "      <td>0.665757</td>\n",
       "      <td>0.588109</td>\n",
       "      <td>0.579148</td>\n",
       "      <td>0.628867</td>\n",
       "      <td>0.631914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spacy</td>\n",
       "      <td>0.445824</td>\n",
       "      <td>0.462882</td>\n",
       "      <td>0.674309</td>\n",
       "      <td>0.683588</td>\n",
       "      <td>0.679176</td>\n",
       "      <td>0.685044</td>\n",
       "      <td>0.599754</td>\n",
       "      <td>0.575873</td>\n",
       "      <td>0.620861</td>\n",
       "      <td>0.631186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Representation  MultinomoalNB MS  MultinomoalNB TS  \\\n",
       "0       Word2Vec CBoW          0.426629          0.433406   \n",
       "1  Word2Vec Skip-Gram          0.428721          0.440138   \n",
       "2       FastText CBoW          0.417486          0.425582   \n",
       "3  FastText Skip-Gram          0.424536          0.435953   \n",
       "4             Doc2Vec          0.440321          0.447780   \n",
       "5         Avg. Vector          0.472526          0.486536   \n",
       "6               Glove          0.415575          0.431405   \n",
       "7               Spacy          0.445824          0.462882   \n",
       "\n",
       "   Logistic Regression MS  Logistic Regression TS  Support Vector Machines MS  \\\n",
       "0                0.606305                0.619905                    0.609261   \n",
       "1                0.625182                0.627729                    0.626592   \n",
       "2                0.582424                0.594978                    0.600255   \n",
       "3                0.614947                0.621179                    0.621452   \n",
       "4                0.633324                0.638464                    0.632778   \n",
       "5                0.620770                0.620451                    0.629139   \n",
       "6                0.658070                0.661936                    0.659798   \n",
       "7                0.674309                0.683588                    0.679176   \n",
       "\n",
       "   Support Vector Machines TS  Stohastic Gradient Descent (SVM) MS  \\\n",
       "0                    0.618086                             0.568869   \n",
       "1                    0.632278                             0.563637   \n",
       "2                    0.614447                             0.478122   \n",
       "3                    0.629367                             0.550900   \n",
       "4                    0.639374                             0.574509   \n",
       "5                    0.628457                             0.619405   \n",
       "6                    0.665757                             0.588109   \n",
       "7                    0.685044                             0.599754   \n",
       "\n",
       "   Stohastic Gradient Descent (SVM) TS  Random Forest MS  Random Forest TS  \n",
       "0                             0.547307          0.554859          0.569505  \n",
       "1                             0.555495          0.575692          0.586426  \n",
       "2                             0.547489          0.500864          0.505822  \n",
       "3                             0.582424          0.552266          0.558042  \n",
       "4                             0.595706          0.588974          0.596070  \n",
       "5                             0.620997          0.625091          0.626092  \n",
       "6                             0.579148          0.628867          0.631914  \n",
       "7                             0.575873          0.620861          0.631186  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ml_adv = pd.DataFrame([['Word2Vec CBoW', w2v_cbow_ms_mnb, w2v_cbow_ts_mnb,\n",
    "                         w2v_cbow_ms_lr, w2v_cbow_ts_lr,\n",
    "                         w2v_cbow_ms_svm, w2v_cbow_ts_svm,\n",
    "                         w2v_cbow_ms_sgd, w2v_cbow_ts_sgd,\n",
    "                         w2v_cbow_ms_rfc, w2v_cbow_ts_rfc],\n",
    "                        ['Word2Vec Skip-Gram', w2v_sg_ms_mnb, w2v_sg_ts_mnb,\n",
    "                         w2v_sg_ms_lr, w2v_sg_ts_lr,\n",
    "                         w2v_sg_ms_svm, w2v_sg_ts_svm,\n",
    "                         w2v_sg_ms_sgd, w2v_sg_ts_sgd,\n",
    "                         w2v_sg_ms_rfc, w2v_sg_ts_rfc],\n",
    "                        ['FastText CBoW', ft_cbow_ms_mnb, ft_cbow_ts_mnb,\n",
    "                         ft_cbow_ms_lr, ft_cbow_ts_lr,\n",
    "                         ft_cbow_ms_svm, ft_cbow_ts_svm,\n",
    "                         ft_cbow_ms_sgd, ft_cbow_ts_sgd,\n",
    "                         ft_cbow_ms_rfc, ft_cbow_ts_rfc],\n",
    "                        ['FastText Skip-Gram', ft_sg_ms_mnb, ft_sg_ts_mnb,\n",
    "                         ft_sg_ms_lr, ft_sg_ts_lr,\n",
    "                         ft_sg_ms_svm, ft_sg_ts_svm,\n",
    "                         ft_sg_ms_sgd, ft_sg_ts_sgd,\n",
    "                         ft_sg_ms_rfc, ft_sg_ts_rfc],\n",
    "                        ['Doc2Vec', dbow_ms_mnb, dbow_ts_mnb,\n",
    "                         dbow_ms_lr, dbow_ts_lr,\n",
    "                         dbow_ms_svm, dbow_ts_svm,\n",
    "                         dbow_ms_sgd, dbow_ts_sgd,\n",
    "                         dbow_ms_rfc, dbow_ts_rfc],\n",
    "                        ['Avg. Vector', avg_vec_ms_mnb, avg_vec_ts_mnb,\n",
    "                         avg_vec_ms_lr, avg_vec_ts_lr,\n",
    "                         avg_vec_ms_svm, avg_vec_ts_svm,\n",
    "                         avg_vec_ms_sgd, avg_vec_ts_sgd,\n",
    "                         avg_vec_ms_rfc, avg_vec_ts_rfc],\n",
    "                        ['Glove', glove_ms_mnb, glove_ts_mnb,\n",
    "                         glove_ms_lr, glove_ts_lr,\n",
    "                         glove_ms_svm, glove_ts_svm,\n",
    "                         glove_ms_sgd, glove_ts_sgd,\n",
    "                         glove_ms_rfc, glove_ts_rfc],\n",
    "                        ['Spacy', spacy_ms_mnb, spacy_ts_mnb,\n",
    "                         spacy_ms_lr, spacy_ts_lr,\n",
    "                         spacy_ms_svm, spacy_ts_svm,\n",
    "                         spacy_ms_sgd, spacy_ts_sgd,\n",
    "                         spacy_ms_rfc, spacy_ts_rfc]],\n",
    "                         columns=['Representation', 'MultinomoalNB MS', 'MultinomoalNB TS',\n",
    "                                  'Logistic Regression MS', 'Logistic Regression TS',\n",
    "                                  'Support Vector Machines MS', 'Support Vector Machines TS', \n",
    "                                  'Stohastic Gradient Descent (SVM) MS', 'Stohastic Gradient Descent (SVM) TS',\n",
    "                                  'Random Forest MS', 'Random Forest TS'])\n",
    "results_ml_adv.to_csv('results_ml_adv.csv')\n",
    "results_ml_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c2309dc-7500-48ca-b96f-f155a06a2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Layer Perceptron BoW:   0.7019650655021834\n",
      "Multi Layer Perceptron TfIdf: 0.6883187772925764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def quick_mlp(rep, test=0.2):\n",
    "    mlp = MLPClassifier(solver='adam', alpha=1e-5, learning_rate='adaptive', early_stopping=True, activation = 'relu', hidden_layer_sizes=(512, 512), random_state=7)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rep, dataframe['sentiment_num'], test_size=test, random_state=7)\n",
    "    mlp.fit(x_train, y_train)\n",
    "    return mlp.score(x_test, y_test)\n",
    "        \n",
    "print(f'Multi Layer Perceptron BoW:   {quick_mlp(bow)}')\n",
    "print(f'Multi Layer Perceptron TfIdf: {quick_mlp(tfidf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699289c8-3ca1-4c19-84b4-5ab7c56a3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam #,Adadelta, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "def regular_dl(x_train, x_test, y_train, y_test, rtype='bow'):\n",
    "    np.random.seed(7)\n",
    "    classes = 3\n",
    "    batch_size = 100\n",
    "    epochs = 20\n",
    "\n",
    "    y_train = to_categorical(y_train, classes)\n",
    "    y_test = to_categorical(y_test, classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    if rtype=='bow':\n",
    "        model.add(Dense(1000, input_shape=(bow.shape[1],)))\n",
    "    else:\n",
    "        model.add(Dense(1000, input_shape=(tfidf.shape[1],)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print (model.summary())\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, workers=6, use_multiprocessing=True, validation_split=0.1, callbacks=[callback])\n",
    "\n",
    "    y_train_predclass =  np.argmax(model.predict(x_train), axis=1)\n",
    "    y_test_predclass =  np.argmax(model.predict(x_test), axis=1)\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(f\"\\n\\nDeep Neural Network - Train accuracy: {accuracy_score(y_train, y_train_predclass)}\")\n",
    "    print(f\"\\nDeep Neural Network - Test accuracy: {accuracy_score(y_test, y_test_predclass)}\")\n",
    "    print(\"\\nDeep Neural Network - Train Classification Report\")\n",
    "    print(classification_report(y_train, y_train_predclass))\n",
    "    print(\"\\nDeep Neural Network - Test Classification Report\")\n",
    "    print(classification_report(y_test, y_test_predclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4541916-744b-4291-8166-530ff080197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 1000)              17965000  \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                25050     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 153       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,490,703\n",
      "Trainable params: 18,490,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "198/198 [==============================] - 17s 84ms/step - loss: 0.8529 - accuracy: 0.6152 - val_loss: 0.7067 - val_accuracy: 0.7085\n",
      "Epoch 2/20\n",
      "198/198 [==============================] - 16s 82ms/step - loss: 0.5756 - accuracy: 0.7787 - val_loss: 0.7523 - val_accuracy: 0.7103\n",
      "Epoch 3/20\n",
      "198/198 [==============================] - 16s 80ms/step - loss: 0.3605 - accuracy: 0.8704 - val_loss: 0.8827 - val_accuracy: 0.7058\n",
      "Epoch 4/20\n",
      "198/198 [==============================] - 16s 80ms/step - loss: 0.2127 - accuracy: 0.9278 - val_loss: 1.0991 - val_accuracy: 0.6921\n",
      "Epoch 5/20\n",
      "198/198 [==============================] - 16s 81ms/step - loss: 0.1353 - accuracy: 0.9575 - val_loss: 1.3362 - val_accuracy: 0.6940\n",
      "687/687 [==============================] - 11s 16ms/step\n",
      "172/172 [==============================] - 3s 15ms/step\n",
      "\n",
      "\n",
      "Deep Neural Network - Train accuracy: 0.8714519650655022\n",
      "\n",
      "Deep Neural Network - Test accuracy: 0.7019650655021834\n",
      "\n",
      "Deep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85      8815\n",
      "           1       0.88      0.91      0.90      6863\n",
      "           2       0.87      0.86      0.87      6306\n",
      "\n",
      "    accuracy                           0.87     21984\n",
      "   macro avg       0.87      0.87      0.87     21984\n",
      "weighted avg       0.87      0.87      0.87     21984\n",
      "\n",
      "\n",
      "Deep Neural Network - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68      2302\n",
      "           1       0.75      0.75      0.75      1719\n",
      "           2       0.69      0.66      0.67      1475\n",
      "\n",
      "    accuracy                           0.70      5496\n",
      "   macro avg       0.70      0.70      0.70      5496\n",
      "weighted avg       0.70      0.70      0.70      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(bow, dataframe['sentiment_num'], test_size=0.2, random_state=7)\n",
    "regular_dl(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dcc59cc-8853-49c9-b262-3d88ce78d92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1000)              23659000  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 153       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,184,703\n",
      "Trainable params: 24,184,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "198/198 [==============================] - 20s 98ms/step - loss: 0.8662 - accuracy: 0.5919 - val_loss: 0.7330 - val_accuracy: 0.6980\n",
      "Epoch 2/20\n",
      "198/198 [==============================] - 20s 101ms/step - loss: 0.5234 - accuracy: 0.7987 - val_loss: 0.7649 - val_accuracy: 0.6858\n",
      "Epoch 3/20\n",
      "198/198 [==============================] - 20s 102ms/step - loss: 0.2777 - accuracy: 0.9072 - val_loss: 0.9771 - val_accuracy: 0.6617\n",
      "Epoch 4/20\n",
      "198/198 [==============================] - 20s 100ms/step - loss: 0.1407 - accuracy: 0.9585 - val_loss: 1.3113 - val_accuracy: 0.6453\n",
      "687/687 [==============================] - 12s 17ms/step\n",
      "172/172 [==============================] - 3s 18ms/step\n",
      "\n",
      "\n",
      "Deep Neural Network - Train accuracy: 0.8135462154294032\n",
      "\n",
      "Deep Neural Network - Test accuracy: 0.6850436681222707\n",
      "\n",
      "Deep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79      8815\n",
      "           1       0.85      0.87      0.86      6863\n",
      "           2       0.89      0.70      0.79      6306\n",
      "\n",
      "    accuracy                           0.81     21984\n",
      "   macro avg       0.83      0.81      0.81     21984\n",
      "weighted avg       0.82      0.81      0.81     21984\n",
      "\n",
      "\n",
      "Deep Neural Network - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68      2302\n",
      "           1       0.73      0.74      0.74      1719\n",
      "           2       0.75      0.53      0.62      1475\n",
      "\n",
      "    accuracy                           0.69      5496\n",
      "   macro avg       0.70      0.67      0.68      5496\n",
      "weighted avg       0.69      0.69      0.68      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tfidf, dataframe['sentiment_num'], test_size=0.2, random_state=7)\n",
    "regular_dl(x_train, x_test, y_train, y_test, rtype='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f20f4b4-3828-4450-bb4d-3fe24cf48b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "def CNN(rep, rtype='bow'):\n",
    "    if rtype == 'bow':\n",
    "        max_features = bow.shape[1]\n",
    "        max_length = bow.shape[1]\n",
    "    else:\n",
    "        max_features = tfidf.shape[1]\n",
    "        max_length = tfidf.shape[1]\n",
    "        \n",
    "    x_train, x_test, y_train, y_test = train_test_split(rep, dataframe['sentiment_num'], test_size=0.2, random_state=7)\n",
    "    print(len(x_train), 'train observations')\n",
    "    print(len(x_test), 'test observations')\n",
    "\n",
    "    x_train = sequence.data_utils.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_test = sequence.data_utils.pad_sequences(x_test, maxlen=max_length)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "\n",
    "    batch_size = 32\n",
    "    embedding_dims = 60\n",
    "    num_kernels = 260\n",
    "    kernel_size = 3\n",
    "    hidden_dims = 300\n",
    "    epochs = 3\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dims, input_length=max_length))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(num_kernels, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1, callbacks=[callback])\n",
    "\n",
    "    y_train_predclass = np.argmax(model.predict(x_train), axis=1)\n",
    "    y_test_predclass = np.argmax(model.predict(x_test), axis=1)\n",
    "    y_train_predclass.shape = y_train.shape\n",
    "    y_test_predclass.shape = y_test.shape\n",
    "\n",
    "    print(f'\\n\\nCNN 1D - Train accuracy: {accuracy_score(y_train, y_train_predclass)}')\n",
    "    print(f'\\nCNN 1D of Training data\\n {classification_report(y_train, y_train_predclass)}')\n",
    "    print(f'\\nCNN 1D - Train Confusion Matrix\\n\\n {pd.crosstab(y_train, y_train_predclass, rownames=[\"Actual\"], colnames=[\"Predicted\"])}')\n",
    "    print(f'\\nCNN 1D - Test accuracy: {accuracy_score(y_test, y_test_predclass)}')\n",
    "    print(f'\\nCNN 1D of Test data\\n {classification_report(y_test, y_test_predclass)}')\n",
    "    print(f'\\nCNN 1D - Test Confusion Matrix\\n\\n {pd.crosstab(y_test, y_test_predclass, rownames=[\"Actual\"], colnames=[\"Predicted\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2a8fa-ea28-4025-8b7c-6db7f75bf50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70e080-58f3-49c0-bf42-5945944f2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN(tfidf, rtype='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ac3721-fe28-4c0b-a837-633003ee83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "def RNN_LSTM(rep, rtype='bow'):\n",
    "    if rtype == 'bow':\n",
    "        max_features = bow.shape[1]\n",
    "        max_length = bow.shape[1]\n",
    "    else:\n",
    "        max_features = tfidf.shape[1]\n",
    "        max_length = tfidf.shape[1]\n",
    "        \n",
    "    batch_size = 64\n",
    "    x_train, x_test, y_train, y_test = train_test_split(rep, dataframe['sentiment_num'], test_size=0.2, random_state=7)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=4, validation_split=0.1, verbose=1, callbacks=[callback])\n",
    "\n",
    "    y_train_predclass = np.argmax(model.predict(x_train), axis=1)\n",
    "    y_test_predclass = np.argmax(model.predict(x_test), axis=1)\n",
    "    y_train_predclass.shape = y_train.shape\n",
    "    y_test_predclass.shape = y_test.shape\n",
    "\n",
    "    print(\"\\n\\nLSTM Bidirectional Sentiment Classification - Train accuracy:\", (round(accuracy_score(y_train, y_train_predclass), 3)))\n",
    "    print(\"\\nLSTM Bidirectional Sentiment Classification of Training data\\n\", classification_report(y_train, y_train_predclass))\n",
    "    print(\"\\nLSTM Bidirectional Sentiment Classification - Train Confusion Matrix\\n\\n\", pd.crosstab(y_train, y_train_predclass, rownames=[\"Actual\"], colnames=[\"Predicted\"]))\n",
    "    print(\"\\nLSTM Bidirectional Sentiment Classification - Test accuracy:\", (round(accuracy_score(y_test, y_test_predclass), 3)))\n",
    "    print(\"\\nLSTM Bidirectional Sentiment Classification of Test data\\n\", classification_report(y_test, y_test_predclass))\n",
    "    print(\"\\nLSTM Bidirectional Sentiment Classification - Test Confusion Matrix\\n\\n\", pd.crosstab(y_test, y_test_predclass, rownames=[\"Actual\"], colnames=[\"Predicted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397335d-e921-42cd-a240-e64c8ee5cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_LSTM(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bf137-fef4-4785-929b-11ca5316d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_LSTM(tfidf, rtype='tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
